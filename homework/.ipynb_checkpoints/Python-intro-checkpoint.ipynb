{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to COMP90042, Web Search and Text Analysis! This is an iPython notebook to introduce you to some of the basics of Python. \n",
    "Python is an interpreted scripting language. Is is interpreted, rather than compiled, which means that you can see the results of a program (or program fragment) simply by using a terminal which can interpret the commands. (Like this one!) This property has made Python popular for certain web apps. On the other hand, because it isn't pre-compiled, Python programs often aren't as fast as equivalent C programs (for example).\n",
    "The \"scripting\" refers to the fact that Python programs are often short, and could equivalently be performed by issuing the various commands directly to an interpreter (which is another property we'll make use of here!).\n",
    "\n",
    "Let's get to it! From the Python terminal below, I'm going to begin by entering a familiar string:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"Hello world!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can \"evaluate\" the \"program\" above, but it's just an r-value, so the statement has no effect. (Often the interpreter will automatically print its output anyway.)\n",
    "\n",
    "Instead, let's instantiate a variable with that string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"Hello world!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is (generally) weakly-typed, which means that we don't need to tell the interpreter in advance what the data type of the variable *sentence* will be (although we can if we wish to). Notice also that the memory for the variable is dynamically allocated.\n",
    "\n",
    "Anyway, what can we do with a string? Well, a string is just a list of characters, so I can print some individual character by deferencing the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print sentence[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can also print a \"slice\" of the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print sentence[6:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something a little more complicated. I can split the string *sentence* up using the in-built method *split()* as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sentence.split()\n",
    "print words[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable *words* now is a list of elements from the string *sentence*, in this case, broken up based on whitespace. One cute property of Python is that lists can also be deferenced from the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print words[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do something non-trivial, we'll need to able to do some file I/O.\n",
    "\n",
    "Let's open the file *words*, which is a list of English words, one per line. (This is open-source, and comes with most \\*nix distributions; you can get a copy from the LMS.) We open files - as in many environments - by initialising a file pointer, in this case, with the *open()* method (read mode takes the *'r'* flag, write mode takes the *'w'* flag, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./words', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few methods for getting the (textual) content of this file. We can load the entire content of the file into a string with *read()*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_file = f.read()\n",
    "print entire_file[0:15]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the file structure is comprised of line breaks, we can instead instantiate a list, where each entry in the list is a line in the file, using *readlines()*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./words', 'r')\n",
    "all_lines = f.readlines()\n",
    "print all_lines[0:5]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use a looping structure to process each line of the file individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./words', 'r')\n",
    "count = 0\n",
    "for line in f:\n",
    "    count += 1\n",
    "print count\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The use of regular expressions in Python requires us to import the *re* package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our word list to build a tool to help us cheat at crossword puzzles. For each element of the list, we are going to use the *search(pattern, string)* method. Let's say that I want to find a 11-letter word whose third letter is \"k\" and whose tenth letter is \"g\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./words', 'r')\n",
    "for line in f:\n",
    "    if re.search('^..k......g.$', line):\n",
    "        print line\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray! What about an 8-letter word whose second and final letters are both \"e\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./words', 'r')\n",
    "for line in f:\n",
    "    if re.search('^.e....e.$', line):\n",
    "        print line,\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... maybe we need to solve a few more clues!\n",
    "\n",
    "Now let's see if we can make a function to do the above for *any* pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_words(pattern):\n",
    "    f = open('./words', 'r')\n",
    "    for line in f:\n",
    "        if re.search(pattern, line):\n",
    "            print line,\n",
    "    f.close()\n",
    "\n",
    "# and use it to find all words with the vowels in order\n",
    "find_matching_words('^.*a.*e.*i.*o.*u.*$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another very useful aspect of Python is access to dictionaries (associative arrays). What if we wished to find out the distribution of final letters of words in the English language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "f = open(\"words\")\n",
    "for line in f:\n",
    "    last_letter = line[-2] # Why -2???\n",
    "    if last_letter not in counts:\n",
    "          counts[last_letter] = 0\n",
    "    counts[last_letter] += 1\n",
    "f.close()\n",
    "for letter in counts.keys():\n",
    "    print letter, counts[letter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An equivalent version makes use of a *defaultdict*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "counts = defaultdict(int) # Any uninstantiated entries are integers (with the value 0)\n",
    "f = open('./words', 'r')\n",
    "for line in f:\n",
    "    counts[line[-2]] += 1\n",
    "f.close()\n",
    "for letter in counts.keys():\n",
    "    print letter, counts[letter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important consideration is when to use a set: a dictionary without values. This is important because it allows near-constant time lookup. For example, consider the following text (Lewis Carroll's Jabberwocky, from Through the Looking Glass (1871)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = ''''Twas brillig, and the slithy toves\n",
    "  Did gyre and gimble in the wabe:\n",
    "All mimsy were the borogoves,\n",
    "  And the mome raths outgrabe.\n",
    "\n",
    "\"Beware the Jabberwock, my son!\n",
    "  The jaws that bite, the claws that catch!\n",
    "Beware the Jubjub bird, and shun\n",
    "  The frumious Bandersnatch!\"\n",
    "\n",
    "He took his vorpal sword in hand:\n",
    "  Long time the manxome foe he sought --\n",
    "So rested he by the Tumtum tree,\n",
    "  And stood awhile in thought.\n",
    "\n",
    "And, as in uffish thought he stood,\n",
    "  The Jabberwock, with eyes of flame,\n",
    "Came whiffling through the tulgey wood,\n",
    "  And burbled as it came!\n",
    "\n",
    "One, two! One, two! And through and through\n",
    "  The vorpal blade went snicker-snack!\n",
    "He left it dead, and with its head\n",
    "  He went galumphing back.\n",
    "\n",
    "\"And, has thou slain the Jabberwock?\n",
    "  Come to my arms, my beamish boy!\n",
    "O frabjous day! Callooh! Callay!'\n",
    "  He chortled in his joy.\n",
    "\n",
    "`Twas brillig, and the slithy toves\n",
    "  Did gyre and gimble in the wabe;\n",
    "All mimsy were the borogoves,\n",
    "  And the mome raths outgrabe. '''\n",
    "tokens = [token.lower() for token in re.split(r'\\W+',text)]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second last line above is known as a list comprehension: it builds up a list by applying an operation (in this case, lower-casing) sequentially to named elements of another list (in this case, the list created by splitting the text on non-alphanumeric characters (like whitespace)). It's a very powerful technique for compactly describing a list; you'll probably be seeing it quite a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we were interested in finding out which tokens in the text aren't elements of the words file above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = [word[:-1] for word in all_lines]\n",
    "unknown_tokens = [token for token in tokens if token not in all_words]\n",
    "unknown_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's subtle, but you might notice a small pause while this evaluates. Effectively, we're checking each token against the entire list of 99K words - the delay is minor here, but could be substantial with a longer text. By converting the word list to a set, we can check the membership much faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = set([word[:-1] for word in all_lines])\n",
    "unknown_tokens = [token for token in tokens if token not in all_words]\n",
    "unknown_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, the lesson ends here: you will need to try some problems for yourself if you want to improve. \n",
    "\n",
    "I recommend the following exercises from NLTK book (http://nltk.org/book/) Chapter 3: 1, 2, and 24\n",
    "\n",
    "The NLTK book is a great reference, and it's pitched at people who've never used Python before. You'll probably also find that the Python documentation (http://docs.pythong.org) is a handy reference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
