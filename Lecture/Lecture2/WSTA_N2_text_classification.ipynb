{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get the corpus we will be using, which is included in NLTK. You will need NLTK and Scikit-learn (as well as their dependencies, in particular scipy and numpy) to run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/daniel/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"reuters\") # if necessary\n",
    "from nltk.corpus import reuters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK sample of the Reuters Corpus contains 10,788 news documents totaling 1.3 million words. The documents have been classified into 90 topics, and is divided into a training and test sets, a split which we will preserve here. Let's look at the counts of texts of the various categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acq 2369\n",
      "alum 58\n",
      "barley 51\n",
      "bop 105\n",
      "carcass 68\n",
      "castor-oil 2\n",
      "cocoa 73\n",
      "coconut 6\n",
      "coconut-oil 7\n",
      "coffee 139\n",
      "copper 65\n",
      "copra-cake 3\n",
      "corn 237\n",
      "cotton 59\n",
      "cotton-oil 3\n",
      "cpi 97\n",
      "cpu 4\n",
      "crude 578\n",
      "dfl 3\n",
      "dlr 175\n",
      "dmk 14\n",
      "earn 3964\n",
      "fuel 23\n",
      "gas 54\n",
      "gnp 136\n",
      "gold 124\n",
      "grain 582\n",
      "groundnut 9\n",
      "groundnut-oil 2\n",
      "heat 19\n",
      "hog 22\n",
      "housing 20\n",
      "income 16\n",
      "instal-debt 6\n",
      "interest 478\n",
      "ipi 53\n",
      "iron-steel 54\n",
      "jet 5\n",
      "jobs 67\n",
      "l-cattle 8\n",
      "lead 29\n",
      "lei 15\n",
      "lin-oil 2\n",
      "livestock 99\n",
      "lumber 16\n",
      "meal-feed 49\n",
      "money-fx 717\n",
      "money-supply 174\n",
      "naphtha 6\n",
      "nat-gas 105\n",
      "nickel 9\n",
      "nkr 3\n",
      "nzdlr 4\n",
      "oat 14\n",
      "oilseed 171\n",
      "orange 27\n",
      "palladium 3\n",
      "palm-oil 40\n",
      "palmkernel 3\n",
      "pet-chem 32\n",
      "platinum 12\n",
      "potato 6\n",
      "propane 6\n",
      "rand 3\n",
      "rape-oil 8\n",
      "rapeseed 27\n",
      "reserves 73\n",
      "retail 25\n",
      "rice 59\n",
      "rubber 49\n",
      "rye 2\n",
      "ship 286\n",
      "silver 29\n",
      "sorghum 34\n",
      "soy-meal 26\n",
      "soy-oil 25\n",
      "soybean 111\n",
      "strategic-metal 27\n",
      "sugar 162\n",
      "sun-meal 2\n",
      "sun-oil 7\n",
      "sunseed 16\n",
      "tea 13\n",
      "tin 30\n",
      "trade 485\n",
      "veg-oil 124\n",
      "wheat 283\n",
      "wpi 29\n",
      "yen 59\n",
      "zinc 34\n"
     ]
    }
   ],
   "source": [
    "for category in reuters.categories():\n",
    "    print(category, len(reuters.fileids(category)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the documents in the corpus are tagged with multiple labels; in this situation, a straightforward approach is to build a classifier for each label. Let's build a classifier to distinguish the most common topic in the corpus, \"acq\" (acqusitions). First, here's some code to build a dataset in preparation for classification using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "def prepare_reuters_data(topic,feature_extractor):\n",
    "    feature_matrix = []\n",
    "    classifications = []\n",
    "    for file_id in reuters.fileids():\n",
    "        feature_dict = feature_extractor(reuters.words(file_id))   \n",
    "        feature_matrix.append(feature_dict)\n",
    "        if topic in reuters.categories(file_id):\n",
    "            classifications.append(topic)\n",
    "        else:\n",
    "            classifications.append(\"not \" + topic)\n",
    "     \n",
    "    vectorizer = DictVectorizer()\n",
    "    dataset = vectorizer.fit_transform(feature_matrix)\n",
    "    return dataset,classifications\n",
    "\n",
    "dataset,classifications = prepare_reuters_data(\"acq\",get_BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code builds a sparse bag of words feature representation (a Python dictionary) for each text in the corpus (which is pre-tokenized) and puts it in a list; a corresponding list of correct classifications is created at the same time. The scikit-learn DictVectorizer class converts Python dictionaries into the scipy sparse matrices which Scikit-learn uses; when working with a single datset, use the fit_transform method to perform the conversion. We can look at the shape of the resulting spare matrix to see how many texts and features we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10788, 41600)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset._shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10788 texts with 41600 features, which is a fairly large feature set. Let's set up a Random Forest classifier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we are using default settings for this classifier. Let's do 10-fold crossvalidation, and looking at the accuracy, recall, precision, and f1-score... (if you are using the latest version of scikit learn (0.18) you will get a depreciation warning when using cross_validation, since cross_validation is included under feature_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/mxnet-env/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation \n",
    "\n",
    "predictions = cross_validation.cross_val_predict(clf, dataset,classifications, cv=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took a little while to build, that is because decision trees don't scale well with large feature sets, and we are building 10 sets of 10 decision tree classifiers, one for each crossvalidation fold. Let's use see what the results look like; Scikit-Learn has build in functions to calculate accuracy and recall/precision/f-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "0.957174638487\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acq       0.90      0.90      0.90      2369\n",
      "    not acq       0.97      0.97      0.97      8419\n",
      "\n",
      "avg / total       0.96      0.96      0.96     10788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def check_results(predictions, classifications):\n",
    "    print(\"accuracy\")\n",
    "    print(accuracy_score(classifications,predictions))\n",
    "    print(classification_report(classifications,predictions))\n",
    "    \n",
    "check_results(predictions, classifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this case, the classifier is not obviously biased towards a particular task, so accuracy and f-score are nearly the same. The performance is quite high, indicating that it is a fairly easy classification task. Let's try to improve performance by removing stopwords and doing lowercasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/daniel/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "accuracy\n",
      "0.958194289952\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acq       0.89      0.92      0.91      2369\n",
      "    not acq       0.98      0.97      0.97      8419\n",
      "\n",
      "avg / total       0.96      0.96      0.96     10788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def get_BOW_lowered_no_stopwords(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "dataset, classification = prepare_reuters_data(\"acq\",get_BOW_lowered_no_stopwords)\n",
    "predictions = cross_validation.cross_val_predict(clf, dataset,classifications, cv=10)\n",
    "check_results(predictions, classifications)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a gain in performance, though it is fairly modest.\n",
    "\n",
    "The default number of decision trees (n_estimators) used in the model is only 10, which is fairly low: lets see if we can find a better number (this will take a while)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "0.943548387097\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acq       0.88      0.85      0.87      2369\n",
      "    not acq       0.96      0.97      0.96      8419\n",
      "\n",
      "avg / total       0.94      0.94      0.94     10788\n",
      "\n",
      "accuracy\n",
      "0.970152020764\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acq       0.94      0.93      0.93      2369\n",
      "    not acq       0.98      0.98      0.98      8419\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10788\n",
      "\n",
      "accuracy\n",
      "0.970615498702\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acq       0.94      0.92      0.93      2369\n",
      "    not acq       0.98      0.98      0.98      8419\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10788\n",
      "\n",
      "accuracy\n",
      "0.97070819429\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acq       0.94      0.92      0.93      2369\n",
      "    not acq       0.98      0.98      0.98      8419\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_to_test = [5,50,100,150]\n",
    "clfs = [RandomForestClassifier(n_estimators=n) for n in n_to_test]\n",
    "for clf in clfs:\n",
    "    predictions = cross_validation.cross_val_predict(clf, dataset,classifications, cv=10)\n",
    "    check_results(predictions, classifications)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, more subclassifiers improved things, though at the cost of speed. Feel free to play around more with this or another classifier to see if you can do better. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
